from __future__ import unicode_literals
import nltk
import string
from hazm import *
from collections import Counter

def get_tokens():
    with open('/DariText.txt', 'r') as shakes:
        text = shakes.read()
        lowers = text.lower()
    #remove the punctuation using the character deletion step of translate
        no_punctuation = lowers.translate( str.maketrans('','',string.punctuation))
        tokens = word_tokenize(no_punctuation)
        return tokens

tokens = get_tokens()
count = Counter(tokens)
print (count.most_common(10))


#StopWord Removal

from nltk.corpus import stopwords
stop_words = stopwords.words=['و','در','به','از','که','این','را','با','است','برای','آن','یک','خود','تا','کرد','بر','هم','نیز','گفت','می‌شود','وی','شد','دارد','ما','اما','یا','شده','باید','هر','آنها','بود','او','دیگر','دو','مورد','می‌کند','شود','کند','وجود','بین' ,'پیش' ,'شده_است' ,'پس' ,'نظر','اگر','همه' ,'یکی' ,'حال' ,'هستند' ,'من' ,'کنند','نیست' ,'باشد' ,'چه','بی' ,'می' ,'بخش' ,'می‌کنند' ,'همین' ,'افزود' ,'هایی' ,'دارند' ,'راه' ,'همچنین' ,'روی' ,'داد' ,'بیشتر' ,'بسیار' ,'سه','داشت' ,'چند' ,'سوی' ,'تنها','هیچ' ,'میان' ,'اینکه' ,'شدن' ,'بعد' ,'جدید' ,'ولی','حتی','کردن' ,'برخی' ,'کردند' ,'می‌دهد','اول' ,'نه' ,'کرده_است' ,'نسبت' ,'بیش','شما' ,'چنین' ,'طور' ,'افراد','تمام','درباره' ,'بار' ,'بسیاری' ,'می‌تواند' ,'کرده' ,'چون' ,'ندارد' ,'دوم' ,'بزرگ' ,'طی' ,'حدود''تهیه','تبدیل','مناسب','زیرا','مشخص','می‌توانند','نزدیک','جریان','روند','بنابراین','می‌دهند','یافت','نخستین','بالا','پنج','ریزی','عالی','چیزی','نخست','بیشتری','ترتیب','شده_بود','خاص','خوبی','خوب','شروع','فرد','کامل','غیر','می‌رود','دهند','آخرین','دادن','جدی','بهترین','شامل','گیرد','بخشی','باشند','تمامی','بهتر','داده_است','حد','نبود','کسانی','می‌کرد','داریم','علیه','می‌باشد','دانست','ناشی','داشتند','دهه','می‌شد','ایشان','آنجا','گرفته_است','دچار','می‌آید','لحاظ','آنکه','داده','بعضی','هستیم','اند','برداری','نباید','می‌کنی','نشست','سهم','همیشه','آمد','اش','وگو','می‌کنم','حداقل','طبق','جا','خواهد_کرد','نوعی','چگونه','رفت','هنگام','فوق','روش','ندارند','سعی','بند','شمار','کلی','کافی','مواجه','همچنان','زیاد','سمت','کوچک','داشته_است','چیز','پشت','آورد','حالا','روبه','سال‌های','دادند','می‌کردند','عهده','نیمه','جایی','دیگران','سی','بروز','یکدیگر','آمده_است','جز','کنم','سپس','کنندگان','خودش','همواره','یافته','شان','صرف','نمی‌شود','رسیدن','چهارم','یابد','متر','ساز','داشته','کرده_بود','باره','نحوه','کردم','تو','شخصی','داشته_باشند','محسوب','پخش','کمی','متفاوت','سراسر','کاملا','داشتن','نظیر','آمده','گروهی','فردی','همچون','خطر','خویش','کدام','دسته','سبب','عین','آوری','متاسفانه','بیرون','دار','ابتدا','شش','افرادی','می‌گویند','سالهای','درون','نیستند','یافته_است','ها','که','پر','خاطرنشان' ,'گاه','جمعی','اغلب','دوباره','می‌یابد','لذا','های','زاده','گرد','اینجا']

tokens = get_tokens()
filtered = [w for w in tokens if not w in stop_words]
count = Counter(filtered)
print (count.most_common(100))

#Stemming



def stem_tokens(tokens, stemmer):
    stemmed = []
    for item in tokens:
        stemmed.append(stemmer.stem(item))
    return stemmed

stemmer = Stemmer()
stemmed = stem_tokens(filtered, stemmer)
count = Counter(stemmed)
print (count.most_common(100))
print('\n')


#print('متن اصلی با termFrequency')
import nltk
import string
import os

from sklearn.feature_extraction.text import TfidfVectorizer
from hazm import Stemmer

path = '/home/mursal/Documents/project/termFrequency/DariFreq/'
token_dict = {}
stemmer = Stemmer()

def stem_tokens(tokens, stemmer):
    stemmed = []
    for item in tokens:
        stemmed.append(stemmer.stem(item))
    return stemmed

def tokenize(text):
    tokens = nltk.word_tokenize(text)
    stems = stem_tokens(tokens, stemmer)
    return stems

for subdir, dirs, files in os.walk(path):
    for file in files:
        file_path = subdir + os.path.sep + file
        shakes = open(file_path, 'r')
        text = shakes.read()
        lowers = text.lower()
        no_punctuation = lowers.translate( str.maketrans('','',string.punctuation))
        token_dict[file] = no_punctuation
        
#this can take some time
tfidf = TfidfVectorizer(tokenize,stop_words)
tfs = tfidf.fit_transform(token_dict.values())

#str.decode("utf-8").replace(u"\u200c", "").encode("utf-8")



print(token_dict)
print(tfs)

str = '/DariText.txt'
response = tfidf.transform([text])
print (response)

feature_names = tfidf.get_feature_names()
for col in response.nonzero()[1]:
    print (feature_names[col], ' - ', response[0, col])
        
    
with open('DariFreq/DariText.txt') as test:
    text = test.read()
    print(text)
    
